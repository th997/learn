version: "3"

# 首次运行参看　hadoop-compose.yml

services:
   hadoop1:
     image: th9976/hbase
     container_name: hadoop1
     hostname: hadoop1
     deploy:
       mode: global
     environment:
       #slaves: hadoop2,hadoop3 # hadoop 2
       workers: hadoop1,hadoop2,hadoop3 # hadoop 3
       regionservers: hadoop1,hadoop2,hadoop3
       start: start-dfs.sh && start-yarn.sh && start-hbase.sh
     depends_on:
       - hadoop2
       - hadoop3
      # 端口 https://blog.csdn.net/shuoyu816/article/details/90573676
     ports:
       - "58020:8020" # hdfs rpc
       - "59870:9870" # nameNode http
       - "59866:9866" # dataNode http
       - "59864:9864" # dataNode rpc
       - "16000:16000" # RegionServer接入
       - "16010:16010" # 集群监控
       - "16020:16020" # 客户端接入
       - "16030:16030" # 节点监控
     tty: true
     stdin_open: true
     volumes:
       - data1:/home/data
     networks:
       default:
         ipv4_address: 172.16.16.61
   
   hadoop2:
     image: th9976/hbase
     container_name: hadoop2
     hostname: hadoop2
     deploy:
       mode: global
     environment:
       regionservers: hadoop1,hadoop2,hadoop3
     tty: true  
     stdin_open: true
     volumes:
       - data2:/home/data
     networks:
       default:
         ipv4_address: 172.16.16.62
         
   hadoop3:
    image: th9976/hbase
    container_name: hadoop3
    hostname: hadoop3
    deploy:
      mode: global
    environment:
      regionservers: hadoop1,hadoop2,hadoop3
    tty: true
    stdin_open: true
    volumes:
      - data3:/home/data
    networks:
      default:
        ipv4_address: 172.16.16.63

volumes:
  data1:
  data2:
  data3:

# 引用已经存在网络
# docker network create --subnet=172.16.16.0/24 mynetwork
networks:
  default:
    external:
      name: mynetwork