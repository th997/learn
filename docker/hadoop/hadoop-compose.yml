version: "3"

# 首次启动
## hadoop1,2,3 hdfs --daemon start journalnode
# hadoop1 hdfs zkfc -formatZK && hdfs namenode -format &&  hdfs --daemon start namenode
# hadoop2,3 hdfs namenode -bootstrapStandby
# restart 

services:
   hadoop1:
     image: th9976/hadoop
     container_name: hadoop1
     hostname: hadoop1
     volumes:
       - data1:/home/data
     deploy:
       mode: global
     environment:
       # slaves: hadoop1,hadoop2,hadoop3 # hadoop 2
       workers: hadoop1,hadoop2,hadoop3 # hadoop 3
       start: start-dfs.sh && start-yarn.sh
     depends_on:
       - hadoop2
       - hadoop3
    # 端口 https://blog.csdn.net/shuoyu816/article/details/90573676
     ports:
       - "58020:8020" # hdfs rpc
       - "59870:9870" # nameNode http
       - "59866:9866" # dataNode http
       - "59864:9864" # dataNode rpc
     tty: true
     stdin_open: true
     networks:
       default:
         ipv4_address: 172.16.16.61
   
   hadoop2:
     image: th9976/hadoop
     container_name: hadoop2
     hostname: hadoop2
     environment:
       a: a
       start: start-dfs.sh && start-yarn.sh
     volumes:
       - data2:/home/data
     deploy:
       mode: global
     tty: true  
     stdin_open: true
     networks:
       default:
         ipv4_address: 172.16.16.62

   hadoop3:
    image: th9976/hadoop
    container_name: hadoop3
    hostname: hadoop3
    environment:
      a: a
      start: start-dfs.sh && start-yarn.sh
    volumes:
      - data3:/home/data
    deploy:
      mode: global    
    tty: true
    stdin_open: true
    networks:
      default:
        ipv4_address: 172.16.16.63

volumes:
  data1:
  data2:
  data3:

# 引用已经存在网络
# docker network create --subnet=172.16.16.0/24 mynetwork
networks:
  default:
    external:
      name: mynetwork