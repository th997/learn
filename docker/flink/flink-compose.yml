version: "3"

# 首次运行参看　hadoop-compose.yml

services:
   hadoop1:
     image: th9976/flink
     container_name: hadoop1
     hostname: hadoop1
     deploy:
       mode: global
     environment:
       #slaves: hadoop2,hadoop3 # hadoop 2
       workers: hadoop1,hadoop2,hadoop3 # hadoop 3
       start: start-all.sh && start-cluster.sh
     depends_on:
       - hadoop2
       - hadoop3
     tty: true
     stdin_open: true
     volumes:
       - data1:/home/data
     networks:
       default:
         ipv4_address: 172.16.16.61
   
   hadoop2:
     image: th9976/flink
     container_name: hadoop2
     hostname: hadoop2
     deploy:
       mode: global
     environment:
       regionservers: hadoop1,hadoop2,hadoop3
     tty: true  
     stdin_open: true
     volumes:
       - data2:/home/data
     networks:
       default:
         ipv4_address: 172.16.16.62
         
   hadoop3:
    image: th9976/flink
    container_name: hadoop3
    hostname: hadoop3
    deploy:
      mode: global
    environment:
      regionservers: hadoop1,hadoop2,hadoop3
    tty: true
    stdin_open: true
    volumes:
      - data3:/home/data
    networks:
      default:
        ipv4_address: 172.16.16.63

volumes:
  data1:
  data2:
  data3:

# 引用已经存在网络
# docker network create --subnet=172.16.16.0/24 mynetwork
networks:
  default:
    external:
      name: mynetwork